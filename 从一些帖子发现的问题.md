比如在保证延迟要求的条件下优化能耗，那并行算法怎么设计？CoEdge: Cooperative DNN Inference With Adaptive Workload Partitioning Over Heterogeneous Edge Devices, TON, 2021就是这样的尝试。

作者：写代码的艾舍尔
链接：https://www.zhihu.com/question/414549247/answer/3060447471
来源：知乎

另外，从[多机并行](https://www.zhihu.com/search?q=多机并行&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3060447471})，到多卡并行，最后到[多核NPU并行](https://www.zhihu.com/search?q=多核NPU并行&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3060447471})，广义上是否仍是分布式，而变化的只是在整个计算过程中通信的媒介和[拓扑链接](https://www.zhihu.com/search?q=拓扑链接&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3060447471})形式呢？如果可以这么看的话，那么像Accelerating Deep Neural Networks on Mobile Multicore NPUs , CGO, 2023这样做多核并行的推理加速工作也有得做啊。

作者：写代码的艾舍尔
链接：https://www.zhihu.com/question/414549247/answer/3060447471
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



所以简单总结下来，你只需要下面在的组合中找到自己擅长的没被深挖的点就好：[optimizing power](https://www.zhihu.com/search?q=optimizing power&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3060447471})/memory footprint/speed/throughput/communication performance of CNN/LLM/GNN for homogeneous/heterogeneous Multiple IoT-Devices/Cards/NPUs/PEs in training/inference stage with online/offline scheduler under the constraint of power/memory footprint/speed/throughput/communication。

作者：写代码的艾舍尔
链接：https://www.zhihu.com/question/414549247/answer/3060447471
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

另外，AI应用的碎片化决定了这个领域的生态是十分多元的，很多组合点确实是存在实际研究价值的，比如[分布式嵌入式](https://www.zhihu.com/search?q=分布式嵌入式&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3060447471})领域，天然就有功耗和内存限制，又天然对性能敏感。



Alpa 的执行过程
Alpa 高度依赖 JAX，它魔改了 XLA （JAX 底层通过 XLA 执行）中的 GSPMD，拿到 XLA 的计算图后，自动对 op 进行切分，生成对应的程序，在每个 worker 上执行。

总结
Alpa 确实是大语言模型并行训练的 SOTA 工作，在理论上突破它还是有相当难度。

